{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f73f76b-35ad-496c-b150-21933761a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "%matplotlib inline\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Text pre-processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbbb40b-7f2b-4094-b8ba-44d269a6537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/train_1000.label\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1dbcbcb-82a4-4b9d-8fa2-7533e2264495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"launch/open_question_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508a1c8b-17bc-4b91-96d2-feb1717f51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fquestions=[]\n",
    "flabel=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f852057e-6909-4419-8d59-2058da36d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DatasetDict is stored in a variable called dataset_dict\n",
    "\n",
    "# Merge train and test datasets\n",
    "merged_df = pd.concat([dataset['train'].to_pandas(), dataset['test'].to_pandas()], ignore_index=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['id', 'annotator1', 'annotator2', 'resolve_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa4eb9a-00ba-46c5-9775-2a2e88058082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 789\n"
     ]
    }
   ],
   "source": [
    "open=0\n",
    "close=0\n",
    "qlist=soup.text.split(\"\\n\")\n",
    "\n",
    "for text in qlist:\n",
    "  j=text.split(\" \",1)\n",
    "\n",
    "\n",
    "  if(len(j)>1):\n",
    "    fquestions.append(j[1])\n",
    "\n",
    "\n",
    "    if(j[0].startswith(\"DESC\")):\n",
    "      flabel.append(0)\n",
    "      open+=1\n",
    "    else:\n",
    "      flabel.append(1)\n",
    "      close+=1\n",
    "print(open,close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1abf0d-b1c1-494a-9565-04d2b828f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fquestions.extend(list(merged_df['question'])[:578])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e80cfa-9b7f-4030-b27d-9dfc31a20b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(578):\n",
    "    flabel.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6630f709-49ba-4ab3-a894-c2b569fc3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "import inflect\n",
    "import contractions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english') and word.lower() not in [\"why\",\"how\",\"when\",\"who\",\"what\",\"can\",\"could\",\"would\",\"should\"]:\n",
    "            new_words.append(word)\n",
    "    words=new_words\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    words=new_words\n",
    "    \n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    words=new_words\n",
    "    \n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    words=new_words\n",
    "    \n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    words=new_words\n",
    "    \n",
    "def normalize(words):\n",
    "    replace_numbers(words)\n",
    "    remove_punctuation(words)\n",
    "    to_lowercase(words)\n",
    "    remove_non_ascii(words)\n",
    "    remove_stopwords(words)\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5a1f57-fa0e-4dd9-8748-235d1b4a3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142697f2-d650-48ab-971a-a04f69ff3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fquestions:\n",
    "    i=normalize(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabdf323-d8d7-411a-b28c-091114e6a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578 1578\n"
     ]
    }
   ],
   "source": [
    "print(len(fquestions),len(flabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb853fa-9b89-4857-95ca-433f362821ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File decompressed successfully\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Step 1: Decompress the .gz file\n",
    "#input_file = r'C:\\Users\\asus\\Desktop\\iknowthis\\finalenv\\CNN TEXT\\GoogleNews-vectors-negative300-SLIM.bin.gz'\n",
    "#output_file = r'C:\\Users\\asus\\Desktop\\iknowthis\\finalenv\\CNN TEXT\\word2vec_model\\GoogleNews-vectors-negative300-SLIM.bin'\n",
    "\n",
    "# Decompress the file\n",
    "#with gzip.open(input_file, 'rb') as f_in:\n",
    "#    with open(output_file, 'wb') as f_out:\n",
    "#        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"File decompressed successfully\")\n",
    "\n",
    "\n",
    "# Step 2: Load the Word2Vec model\n",
    "model_path = 'word2vec_model/GoogleNews-vectors-negative300-SLIM.bin'\n",
    "embed_lookup = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5c8427c-ddc3-455d-abdd-b2aa67920993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "qse = pd.Series(fquestions)\n",
    "lse =pd.Series(flabel)\n",
    "max_len=300\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(qse, lse, shuffle=True, test_size=0.2,stratify=lse)\n",
    "\n",
    "max_words = 10000\n",
    "print(type(xtrain),type(xtest))\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "xtrain_seq = tokenizer.texts_to_sequences(xtrain)\n",
    "xtest_seq = tokenizer.texts_to_sequences(xtest)\n",
    "\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#print('text example:', xtrain[0])\n",
    "#print('sequence of indices(before padding):', xtrain_seq[0])\n",
    "#print('sequence of indices(after padding):', xtrain_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f19c44-209d-46da-9351-1b14f120e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim=300\n",
    "vocab_len=3543\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af11f0c-1888-4664-91f3-f146b32ef3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
    "for word, idx in word_index.items():\n",
    "    if idx < vocab_len:  # Ensure the index is within bounds\n",
    "        if word in embed_lookup:\n",
    "            vector = embed_lookup[word]\n",
    "            embedding_matrix[idx] = vector\n",
    "    else:\n",
    "        print(f\"Index {idx} for word '{word}' is out of bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942b9e3d-0577-4065-8bcf-469a98351905",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_pad = np.clip(xtrain_pad, 0, vocab_len - 1)\n",
    "xtest_pad = np.clip(xtest_pad, 0, vocab_len - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8894535d-ab63-48a8-8a50-f7fc52a74aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      2\u001b[0m maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m      3\u001b[0m vocab_size\u001b[38;5;241m=\u001b[39mvocab_len\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "maxlen=10000\n",
    "vocab_size=vocab_len\n",
    "from tensorflow.keras.layers import Input, Embedding\n",
    "EMBEDDING_DIM=300\n",
    "embedding_dim=300\n",
    "embedding_layer = Embedding(vocab_len, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights=[embedding_matrix], \n",
    "                            \n",
    "                            trainable=False)\n",
    "print('Trainning embedding model.')\n",
    "\n",
    "# Testing embedding\n",
    "#print(embedding_matrix[100])\n",
    "#print(word_index.get('any'))\n",
    "#print(data_train[100])\n",
    "#print(data_test[100])\n",
    "\n",
    "\n",
    "# Attention\n",
    "'''\n",
    "If SINGLE_ATTENTION_VECTOR = true, \n",
    "the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "'''\n",
    "from keras.layers import multiply\n",
    "MAX_LENGTH=10000\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "TIME_STEPS = MAX_LENGTH\n",
    "\n",
    "def __attention3DBlock__(inputs):\n",
    "    '''\n",
    "    Input shape = (batch_size, time_steps, input_dim)\n",
    "    '''\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2,1))(inputs)  # (batch_size, input_dim, time_steps)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # (batch_size, input_dim, time_steps)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a) # (batch_size, input_dim, time_steps)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a) # (batch_size, input_dim, time_steps)\n",
    "    a_probs = Permute((2,1), name='attention_vector')(a) # (batch_size, time_steps, input_dim)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attenton_mul') # (batch_size, time_steps, input_dim)\n",
    "\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "# Model\n",
    "from keras import regularizers\n",
    "from keras.layers import multiply\n",
    "#import self_attention\n",
    "\n",
    "# Parameters\n",
    "'''\n",
    "n_x: hidden state size of GRU\n",
    "regularization: L2 normalization\n",
    "optimization: AdaGrad/Adam\n",
    "time_steps = MAX_LENGTH = 270`\n",
    "'''\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "l2_regularization = 0.001\n",
    "learning_rate = 0.001\n",
    "n_x = 32   \n",
    "epochs = 10\n",
    "time_steps = MAX_LENGTH\n",
    "\n",
    "# Build model\n",
    "print (\"Build model...\")\n",
    "sequence_input = Input(shape=(time_steps,), dtype='float32')\n",
    "print('Sequence input is:', sequence_input) \n",
    "embedded_sequences = embedding_layer(sequence_input) \n",
    "print('Embedding layer is:', embedded_sequences) \n",
    "\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 4, 5],\n",
    "                  vocab_size=[vocab_size],\n",
    "                  embedding_dim=[embedding_dim],\n",
    "                  maxlen=[maxlen])\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                        epochs=epochs, batch_size=10,\n",
    "                        verbose=False)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                          cv=4, verbose=1, n_iter=5)\n",
    "\n",
    "L = (GRU(n_x,\n",
    "            activation='tanh', \n",
    "            dropout=0.2, \n",
    "            recurrent_dropout=0.2, \n",
    "            return_sequences=True,\n",
    "            kernel_initializer='he_uniform',\n",
    "            name='Pre-GRU'))(embedded_sequences)\n",
    "print('GRU is:', L) # (batch_size, time_steps, units=32*2)\n",
    "\n",
    "L = __attention3DBlock__(L) \n",
    "print ('Attention layer is:', L)\n",
    "\n",
    "L = GRU(n_x, \n",
    "        activation='tanh', \n",
    "        kernel_regularizer=regularizers.l2(0.001))(L)\n",
    "print ('Post GRU is:', L)\n",
    "L = Dense(2, \n",
    "          activation='softmax', \n",
    "          kernel_regularizer=regularizers.l2(0.001))(L)\n",
    "print('Dense layer is:', L)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=L)\n",
    "\n",
    "# Optimization and compile\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "print('Begin compiling...')\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Begin training\n",
    "model.fit(xtrain_pad, \n",
    "          np.asarray(ytrain), \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=2,\n",
    "          validation_data=(xtest_pad, np.asarray(ytest)))\n",
    "score = model.evaluate(xtest_pad, np.asarray(ytest), batch_size=batch_size)\n",
    "print ('The evaluation is: ', score)\n",
    "\n",
    "# Evaluate testing set\n",
    "#test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Save model\n",
    "print ('Saving model...')\n",
    "model.save('CNN-GRU-Turkish corpus-200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4be9aa2-4b7f-4644-b2b1-a09089f259c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement self_attention (from versions: none)\n",
      "ERROR: No matching distribution found for self_attention\n"
     ]
    }
   ],
   "source": [
    "%pip install self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d66a95-6bf3-4363-8e92-0f4614785a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,062,900</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ Pre-GRU (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,064</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ permute_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Pre-GRU[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ permute_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">250,500</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_vec (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_mul (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Pre-GRU[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
       "│                               │                           │                 │ attention_vec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,336</span> │ attention_mul[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m300\u001b[0m)          │       \u001b[38;5;34m1,062,900\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ Pre-GRU (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │          \u001b[38;5;34m32,064\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ permute_3 (\u001b[38;5;33mPermute\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ Pre-GRU[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ permute_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m)           │         \u001b[38;5;34m250,500\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_vec (\u001b[38;5;33mPermute\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_mul (\u001b[38;5;33mMultiply\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ Pre-GRU[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
       "│                               │                           │                 │ attention_vec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │           \u001b[38;5;34m6,336\u001b[0m │ attention_mul[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │              \u001b[38;5;34m66\u001b[0m │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,351,866</span> (5.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,351,866\u001b[0m (5.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">288,966</span> (1.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m288,966\u001b[0m (1.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,062,900</span> (4.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,062,900\u001b[0m (4.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 - 55s - 865ms/step - accuracy: 0.7590 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 2/10\n",
      "63/63 - 35s - 549ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 3/10\n",
      "63/63 - 34s - 545ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 4/10\n",
      "63/63 - 35s - 553ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 5/10\n",
      "63/63 - 35s - 549ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 6/10\n",
      "63/63 - 35s - 548ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 7/10\n",
      "63/63 - 36s - 571ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 8/10\n",
      "63/63 - 34s - 547ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 9/10\n",
      "63/63 - 36s - 566ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "Epoch 10/10\n",
      "63/63 - 34s - 545ms/step - accuracy: 0.7600 - loss: nan - val_accuracy: 0.7000 - val_loss: nan\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.7190 - loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation score: [nan, 0.699999988079071]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Permute, Reshape, Lambda, RepeatVector, multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters and configurations\n",
    "maxlen = 500\n",
    "vocab_size = vocab_len  # Adjust this to your vocabulary size\n",
    "embedding_dim = 300\n",
    "EMBEDDING_DIM = 300\n",
    "TIME_STEPS = maxlen\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "n_x = 32\n",
    "\n",
    "# Example embedding matrix and data\n",
    "embedding_matrix = np.random.rand(vocab_size, EMBEDDING_DIM)\n",
    "xtrain_pad = np.random.randint(0, vocab_size, (1000, maxlen))  # Dummy training data\n",
    "xtest_pad = np.random.randint(0, vocab_size, (200, maxlen))    # Dummy test data\n",
    "ytrain = np.random.randint(0, 2, (1000, 2))                    # Dummy training labels\n",
    "ytest = np.random.randint(0, 2, (200, 2))                      # Dummy test labels\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(vocab_size, \n",
    "                            EMBEDDING_DIM, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=maxlen, \n",
    "                            trainable=False)\n",
    "\n",
    "# Attention mechanism\n",
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: tf.keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "# Build model\n",
    "sequence_input = Input(shape=(TIME_STEPS,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# First GRU layer\n",
    "L = GRU(n_x, activation='tanh', dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='he_uniform', name='Pre-GRU')(embedded_sequences)\n",
    "\n",
    "# Attention layer\n",
    "L = attention_3d_block(L)\n",
    "\n",
    "# Second GRU layer\n",
    "L = GRU(n_x, activation='tanh', kernel_regularizer=regularizers.l2(0.001))(L)\n",
    "\n",
    "# Output layer\n",
    "L = Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.001))(L)\n",
    "\n",
    "# Define and compile model\n",
    "model = Model(inputs=sequence_input, outputs=L)\n",
    "opt = Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(xtrain_pad, ytrain, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(xtest_pad, ytest))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(xtest_pad, ytest, batch_size=batch_size)\n",
    "print('Test evaluation score:', score)\n",
    "\n",
    "# Save the model\n",
    "model.save('CNN-GRU-Turkish-corpus-200d.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69dccf4b-0a74-46e5-9fcc-d727b4f40186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6]\n",
      "Tokenized and Padded Sequence: [[2 3 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Predictions: [[nan nan]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Example embedding matrix and word index\n",
    "# Assuming `embedding_matrix` and `word_index` are defined as in your training script\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "word_index = {w: i for i, w in enumerate(['<PAD>', 'do', 'you', 'believe', 'in', 'god'], 1)}  # Example word index\n",
    "\n",
    "# Function to convert text to sequence using the word index\n",
    "def text_to_sequence(text, word_index):\n",
    "    words = text_to_word_sequence(text)\n",
    "    sequence = [word_index.get(word, 0) for word in words]  # 0 for unknown words\n",
    "    return sequence\n",
    "\n",
    "# Example question\n",
    "question = \"do you believe in god\"\n",
    "\n",
    "# Convert the question to a sequence\n",
    "q_seq = text_to_sequence(question, word_index)\n",
    "print(q_seq)\n",
    "\n",
    "# Pad the sequence\n",
    "q_pad = pad_sequences([q_seq], maxlen=maxlen, padding='post')\n",
    "\n",
    "print(\"Tokenized and Padded Sequence:\", q_pad)\n",
    "\n",
    "# Load the model (if not already loaded)\n",
    "#model = load_model('CNN-GRU-Turkish-corpus-200d.h5')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(q_pad)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e72ecfb8-5599-4c60-a7e1-861c2914c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and Padded Sequence: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "Predictions: [[nan nan]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Example question\n",
    "question = \"do you believe in god\"\n",
    "\n",
    "q=[]\n",
    "for i in question.split():\n",
    "    q.append(embed_lookup(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca053f47-4c18-4401-a8e7-acd4702bb3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
